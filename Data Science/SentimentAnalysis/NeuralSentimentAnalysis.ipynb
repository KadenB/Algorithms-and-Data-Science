{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings # \n",
    "import numpy as np\n",
    "import pandas as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Dropout, LSTM, GRU, AveragePooling1D, GlobalAveragePooling1D, BatchNormalization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "import itertools\n",
    "import os\n",
    "import string\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I accidentally created a sentiment evaluation model, I apologize for misreading, but I have included\n",
    "the trained model in the submisison. My computer is immensley slow when running a NN so I only used 50epochs\n",
    "given that I used a pretrained word embedding model and the accuracy for this was around 83%\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self,epochs,batch_size):\n",
    "        ## Initialize values for our training and testing set\n",
    "        self.train = None # will hold the training set\n",
    "        self.train_size = None \n",
    "        self.test = None\n",
    "        self.test_size = None\n",
    "        \n",
    "        self.train_lables = None\n",
    "        self.test_lables = None\n",
    "        \n",
    "        ### get our sentence informatioon\n",
    "        self.vocab_size = None\n",
    "        self.max_sentence_len = None\n",
    "        self.mean_sentence_len = None\n",
    "        \n",
    "        ## Prepare to have our input data ###\n",
    "        self.prepared_train = None\n",
    "        self.prepared_test = None\n",
    "        \n",
    "        self.embedding_matrix = None\n",
    "        \n",
    "        \n",
    "        ## Set our epochs and batch size for use later###\n",
    "        self.num_epochs = epochs\n",
    "        self.batch = batch_size\n",
    "        \n",
    "        self.t = Tokenizer()\n",
    "            \n",
    "        \n",
    "    def generate_dataframe(self,directory,partition,label):\n",
    "        ## Binary encode our label##\n",
    "        review_dic = {}\n",
    "        if label == \"neg\":\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "      # Generate a dataframe based on our data where we attach  the imbd id with the review and the pos or neg label\n",
    "        for x in os.listdir(directory):\n",
    "            file = open(directory+\"/\"+ str(x))\n",
    "            text  = file.read()\n",
    "            name = str(x)\n",
    "            review_dic[name] = [text,partition,label]\n",
    "\n",
    "        data = pd.DataFrame(review_dic).T\n",
    "        data.reset_index(inplace= True)\n",
    "        data.rename(columns={0:'Text',\n",
    "                              1:'Partition',\n",
    "                              2:'Label',\n",
    "                              \"index\": \"ID\"}, \n",
    "                     inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Generate all our data from the IMBD file\n",
    "    def get_all_data(self):\n",
    "        \n",
    "        # Gather all datasets # \n",
    "        train_pos =self.generate_dataframe(\"directory",\"train\",\"pos\")\n",
    "        train_neg = self.generate_dataframe(\"directory",\"train\",\"neg\")\n",
    "\n",
    "        test_pos= self.generate_dataframe(\"directory",\"test\",\"pos\")\n",
    "        test_neg =self.generate_dataframe(\"directory",\"test\",\"neg\")\n",
    "        \n",
    "        \n",
    "        # concatenate the training and the testing set into \n",
    "        \n",
    "        self.train= pd.concat([train_pos,train_neg],ignore_index= True)\n",
    "        self.test = pd.concat([test_pos,test_neg],ignore_index= True)\n",
    "        \n",
    "        print(self.train.head())\n",
    "        # Assign our labels\n",
    "        self.train_lables = self.train.Label \n",
    "        self.test_lables = self.test.Label\n",
    "        \n",
    "        \n",
    "        # Drop labels\n",
    "        self.train = self.train.drop([\"Label\"],axis = 1)\n",
    "        self.test =  self.test.drop([\"Label\"],axis = 1)\n",
    "        \n",
    "        # Get our sizes \n",
    "        self.train_size = self.train.shape[0]\n",
    "        self.test_size = self.test.shape[0]\n",
    "        \n",
    "      \n",
    "        return \"Generated Train and Test set for Processing: Train set size is \" + str(self.train_size) + \" Test set size is \"+str(self.test_size)\n",
    "        \n",
    "    \n",
    "    ### Now for the preprocessing steps ###\n",
    "    #\n",
    "    def convert_to_lower(self):\n",
    "        \n",
    "        self.train.Text = self.train.Text.apply(lambda x: x.lower())\n",
    "        self.test.Text = self.test.Text.apply(lambda x: x.lower()) \n",
    "        \n",
    "    def strip_punc(self):\n",
    "       \n",
    "        self.train.Text = self.train.Text.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "        self.test.Text = self.test.Text.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "    \n",
    " \n",
    "    \n",
    "    def get_max_sequence(self):\n",
    "        # find the largest,mean,median sentence in the train dataset\n",
    "        \n",
    "        self.train[\"Words\"] = self.train.Text.apply(lambda x: x.split())\n",
    "        self.train[\"WC\"] = self.train.Words.apply(lambda x: len(x)) # realize here I couldve just done it directly with len(x.split())- perhaps was a little tired !\n",
    "        \n",
    "        max_sentence = self.train.WC.max()\n",
    "        median_sentence = self.train.WC.median()\n",
    "        mean_sentence = self.train.WC.mean()\n",
    "        \n",
    "        self.max_sentence_len = int(max_sentence)\n",
    "        self.median_sentence_len = int(median_sentence)\n",
    "        self.mean_sentence_len = int(mean_sentence)\n",
    "        \n",
    "     \n",
    "    def get_tokens_and_encode(self): \n",
    "        ## Encode our data and pad for input into the nueral net\n",
    "        #t = Tokenizer()\n",
    "        self.t.fit_on_texts(self.train_text)\n",
    "        #t.word_index\n",
    "        self.vocab_size = int(len(self.t.word_index)*1.30) # here we can get out vocabulary size\n",
    "        \n",
    "        print(\"Vocab size %s\"% self.vocab_size)\n",
    "    \n",
    "        # Tokenize, sequence, and pad the data\n",
    "        \n",
    "        encoded_train = self.t.texts_to_sequences(self.train.Text)\n",
    "        encoded_test = self.t.texts_to_sequences(self.test.Text)\n",
    "       \n",
    "        # Here I've decided to use the mean sentence length as the max length to accept\n",
    "        padded_train_text = pad_sequences(encoded_train, maxlen=self.mean_sentence_len, padding='post')\n",
    "        padded_test_text = pad_sequences(encoded_test, maxlen=self.mean_sentence_len, padding='post')\n",
    "        \n",
    "        # Now we have our prepared data for the model\n",
    "        self.prepared_train = padded_train_text\n",
    "        self.prepared_test = padded_test_text\n",
    "        \n",
    "        print(\"Prepared all data for modeling\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_pretrained(self):\n",
    "        # load the whole embedding into memory - taken directly # glove only hads 100 dimensions\n",
    "        embeddings_index = {}\n",
    "        f = open('glove.6B.100d.txt')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        self.embeddings_index = embeddings_index\n",
    "        print('Loaded %s word vectors.' % len(self.embeddings_index))\n",
    "\n",
    "    def create_embedding_matrix(self):\n",
    "        # taken direct from example  just to load data for the glove embedding\n",
    "        embedding_matrix = np.zeros((self.vocab_size, 100))\n",
    "        \n",
    "        for word, i in self.t.word_index.items():\n",
    "\n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "\n",
    "            if embedding_vector is not None :\n",
    "                \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        \n",
    "        print(\"Embeddings matrix unique vals %s \" % np.unique(self.embedding_matrix))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_model(self,model_name,pretrained = \"True\"):\n",
    "        # create our Sentiment classificaiton model\n",
    "        \n",
    "        model = Sequential()\n",
    "        # create our embedding layer with our embedding weights from the glove pretrained model\n",
    "        e = Embedding(self.vocab_size, 100, weights=[self.embedding_matrix], input_length=self.mean_sentence_len, trainable=False)\n",
    "        \n",
    "        model.add(e)\n",
    "        #model.add(GRU(200))\n",
    "        model.add(LSTM(100,return_sequences =True))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        #model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid')) # add our dense layer for output\n",
    "        \n",
    "        # compile \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "              optimizer=  optimizers.adam(lr = .0001, decay = 1e-4),\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        model.fit\n",
    "        print(model.summary())\n",
    "        \n",
    "        model.fit(self.prepared_train, self.train_lables,\n",
    "              batch_size=self.batch,\n",
    "              epochs=self.num_epochs,\n",
    "              validation_data=(self.prepared_test, self.test_lables),\n",
    "              shuffle=True)\n",
    "        \n",
    "        scores = model.evaluate(self.prepared_test, self.test_lables, verbose=1)\n",
    "        print('Test loss:', scores[0])\n",
    "        print('Test accuracy:', scores[1])\n",
    "        model.save(\"directory" + str(model_name))\n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessing(20,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ID                                               Text Partition  \\\n",
      "0   4715_9.txt  For a movie that gets no respect there sure ar...     train   \n",
      "1  12390_8.txt  Bizarre horror movie filled with famous faces ...     train   \n",
      "2   8329_7.txt  A solid, if unremarkable film. Matthau, as Ein...     train   \n",
      "3   9063_8.txt  It's a strange feeling to sit alone in a theat...     train   \n",
      "4  3092_10.txt  You probably all already know this by now, but...     train   \n",
      "\n",
      "  Label  \n",
      "0     1  \n",
      "1     1  \n",
      "2     1  \n",
      "3     1  \n",
      "4     1  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generated Train and Test set for Processing: Train set size is 25000 Test set size is 25000'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.convert_to_lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.strip_punc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.get_max_sequence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 158457\n",
      "Prepared all data for modeling\n"
     ]
    }
   ],
   "source": [
    "prep.get_tokens_and_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "prep.get_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings matrix unique vals [-3.67919993 -3.49710011 -3.42389989 ...  2.94059992  2.97129989\n",
      "  3.18149996] \n"
     ]
    }
   ],
   "source": [
    "prep.create_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 232)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.mean_sentence_len\n",
    "prep.prepared_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 232, 100)          15845700  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 232, 100)          80400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 16,006,601\n",
      "Trainable params: 160,901\n",
      "Non-trainable params: 15,845,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 938s 38ms/step - loss: 0.6837 - acc: 0.5378 - val_loss: 0.6659 - val_acc: 0.5614\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 1295s 52ms/step - loss: 0.6240 - acc: 0.6625 - val_loss: 0.5501 - val_acc: 0.7435\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 854s 34ms/step - loss: 0.5621 - acc: 0.7356 - val_loss: 0.5334 - val_acc: 0.7547\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 1031s 41ms/step - loss: 0.5264 - acc: 0.7598 - val_loss: 0.5025 - val_acc: 0.7710\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 1084s 43ms/step - loss: 0.4833 - acc: 0.7761 - val_loss: 0.4681 - val_acc: 0.7826\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 1176s 47ms/step - loss: 0.4693 - acc: 0.7836 - val_loss: 0.4510 - val_acc: 0.7909\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 1027s 41ms/step - loss: 0.4530 - acc: 0.7910 - val_loss: 0.4408 - val_acc: 0.7965\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 1191s 48ms/step - loss: 0.4421 - acc: 0.7978 - val_loss: 0.4611 - val_acc: 0.7785\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 1095s 44ms/step - loss: 0.4351 - acc: 0.8024 - val_loss: 0.4217 - val_acc: 0.8088\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 1136s 45ms/step - loss: 0.4238 - acc: 0.8086 - val_loss: 0.5019 - val_acc: 0.7740\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 1551s 62ms/step - loss: 0.4159 - acc: 0.8152 - val_loss: 0.4062 - val_acc: 0.8174\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 1317s 53ms/step - loss: 0.4027 - acc: 0.8227 - val_loss: 0.4255 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 1073s 43ms/step - loss: 0.3979 - acc: 0.8234 - val_loss: 0.3910 - val_acc: 0.8268\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 1057s 42ms/step - loss: 0.3917 - acc: 0.8262 - val_loss: 0.3872 - val_acc: 0.8269\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 931s 37ms/step - loss: 0.3853 - acc: 0.8306 - val_loss: 0.3788 - val_acc: 0.8340\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 888s 36ms/step - loss: 0.3798 - acc: 0.8327 - val_loss: 0.3896 - val_acc: 0.8250\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 1002s 40ms/step - loss: 0.3739 - acc: 0.8354 - val_loss: 0.3707 - val_acc: 0.8365\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 1042s 42ms/step - loss: 0.3669 - acc: 0.8420 - val_loss: 0.3665 - val_acc: 0.8410\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 930s 37ms/step - loss: 0.3645 - acc: 0.8428 - val_loss: 0.3751 - val_acc: 0.8312\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 990s 40ms/step - loss: 0.3649 - acc: 0.8420 - val_loss: 0.3874 - val_acc: 0.8339\n",
      "25000/25000 [==============================] - 316s 13ms/step\n",
      "Test loss: 0.38735788912773134\n",
      "Test accuracy: 0.83388\n"
     ]
    }
   ],
   "source": [
    "prep.create_model(\"EmbeddingLSTM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.fit_on_texts(train.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
