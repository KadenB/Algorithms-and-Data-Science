{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Assignment 2\n",
    "\n",
    "### Part 1 (FSAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "1.- Define a deterministic finite-state automaton that accepts strings that have an odd number of 0’s and any number of 1’s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Uploaded as picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.- Implement a regular expression stemmer that can process the following text. \n",
    "\n",
    "*Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemm', 'usual', 'refer', 'to', 'a', 'crude', 'heuristic', 'proces', 'that', 'chop', 'off', 'the', 'end', 'of', 'word', 'in', 'the', 'hope', 'of', 'achiev', 'thi', 'goal', 'correct', 'most', 'of', 'the', 'time', ',', 'and', 'often', 'includ', 'the', 'removal', 'of', 'derivational', 'affix', '.', 'Lemmatization', 'usual', 'refer', 'to', 'do', 'thing', 'proper', 'with', 'the', 'use', 'of', 'a', 'vocabulary', 'and', 'morphological', 'analysi', 'of', 'word', ',', 'normal', 'aim', 'to', 'remove', 'inflectional', 'ending', 'on', 'and', 'to', 'return', 'the', 'base', 'or', 'dictionary', 'form', 'of', 'a', 'word', ',', 'which', 'i', 'known', 'a', 'the', 'lemma', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "raw =\"\"\" Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, \n",
    "and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove \n",
    "inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "#print(tokens)\n",
    "def stem_word(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    \n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "    \n",
    "print([stem_word(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.- Expand the grammar grammar1.cfg so that it also parses the sentence\n",
    "\n",
    "*John said to Bob that Mary saw a man with a telescope*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John)\n",
      "  (VP\n",
      "    (V said)\n",
      "    (PP (P to) (NP Bob))\n",
      "    (CP\n",
      "      (C that)\n",
      "      (S\n",
      "        (NP Mary)\n",
      "        (VP\n",
      "          (V saw)\n",
      "          (NP (Det a) (N man))\n",
      "          (PP (P with) (NP (Det a) (N telescope))))))))\n",
      "(S\n",
      "  (NP John)\n",
      "  (VP\n",
      "    (V said)\n",
      "    (PP (P to) (NP Bob))\n",
      "    (CP\n",
      "      (C that)\n",
      "      (S\n",
      "        (NP Mary)\n",
      "        (VP\n",
      "          (V saw)\n",
      "          (NP\n",
      "            (Det a)\n",
      "            (N man)\n",
      "            (PP (P with) (NP (Det a) (N telescope)))))))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nltk.download('grammarA.cfg')\n",
    "from nltk import CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "#cp = parse.load_parser('grammars/book_grammars/feat0.fcfg', trace=1)\n",
    "\n",
    "g = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP \n",
    "  VP -> V NP | V NP PP | V PP CP\n",
    "  CP -> C S\n",
    "  C -> \"that\"\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"| \"said\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\" \n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\" | \"to\" | \n",
    "  \"\"\")\n",
    "#rd = RecursiveDescentParser(g)\n",
    "sent = \"John said to Bob that Mary saw a man with a telescope\"\n",
    "tokens = sent.split()\n",
    "\n",
    "#trees = rd.parse(tokens)\n",
    "p = nltk.ChartParser(g)\n",
    "p.parse(tokens)\n",
    "\n",
    "for tree in p.parse(tokens):\n",
    "    print(tree)\n",
    "    tree.draw()\n",
    "\n",
    "    \n",
    "    #for tree in trees:\n",
    "   # print(tree)\n",
    "    #tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 2 (Wordnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this assignment you will be creating a program for learning languages! We will print a fairy tale, and propose a simple test to check if the language learner knows the target language!\n",
    "\n",
    "There is a file called `little-red-riding-hood-clean-5lines.txt`, which, as the name suggests, contains the story *Little Red Riding Hood*.\n",
    "\n",
    "Your job is to do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1st step:\n",
    "\n",
    " * Open and load file\n",
    " * Read text and remove punctuation (Remember the second *Scientific Programming* class)\n",
    " * Tokenize and lemmatize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The story only contains 5 lines. But each line can contain conversations, which are concatenated together in a single line.\n",
    "\n",
    "##### Note: If somebody wants to work with a file that contains more lines, you can use the file called `little-red-riding-hood-clean.txt`, which has more lines (conversations were not concatenated together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2nd step:\n",
    "\n",
    "Assuming you opened the file with 5 lines, for each paragraph you have to do the following:\n",
    "\n",
    " * Get synsets for all words (in English)\n",
    " \n",
    " * For each word, generate lemmas in a target language (and store them)\n",
    " \n",
    " * Choose 5 random words (make sure they have a target lemma)\n",
    " \n",
    " * For each of those random words, ask something that looks like this:\n",
    " \n",
    "    * How youd you say the word `RANDOM_WORD` in Bulgarian (I use bulgarian as example, this can be any language of your choice)?\n",
    "    * Propose, then one correct lemma (from Wordnet) and other 4 random words (it doesn't matter where you get this random words, but they should be different in each test)\n",
    "    \n",
    "When you make your experiments, please tell me the target language you are using, so that I test it with that language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['open', 'day', 'look', 'open', 'wine']\n",
      "Random [['åbne'], ['åben'], ['åbenlys'], ['dag', 'døgn'], ['udseende'], ['kigge'], [\"se_''ud\"], ['lede', 'søge'], ['åbne'], ['åben'], ['åbenlys'], ['druevin', 'vin', 'vinsort']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Experiment 1 \\n\\n     random_five = [\\'latch\\', \\'suppose\\', \\'take\\', \\'afraid\\', \\'got\\']\\n     \\n     in danish ( from google translate): \"låsen, formode, tage, bange, fik \"\\n        \\n     the only lemma matches for exp1 was bange.\\n     \\n    Exp 2\\n    \\n     random_five = \\'saw\\', \\'got\\', \\'anything\\', \\'suppose\\', \\'open\\'\\n     \\n     in danish = :sav, fik, hvad som helst, formode, aben\\n\\n     The lemmas that mached were save for saw, and aben for open. \\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "from nltk import pos_tag\n",
    "\n",
    "file = open('little-red-riding-hood-clean-5lines.txt', encoding=\"utf8\") \n",
    "\n",
    "def removePunctuation(word):\n",
    "    return re.sub(\"[^\\w\\s\\-\\']\", \"\", word)\n",
    "\n",
    "rh =[removePunctuation(line.strip('\\n').lower()) for line in file]   \n",
    "\n",
    "s = str(rh).strip(\"[\").strip(\"'\")\n",
    "tokens = s.split() # list of all words in txt\n",
    "\n",
    "### Create Lematized form ###\n",
    "word_lem = WordNetLemmatizer()\n",
    "lem = [word_lem.lemmatize(word, tag[0].lower()) if tag[0].lower() in ['a', 'n', 'v'] else word_lem.lemmatize(word) for word ,tag in pos_tag(tokens)]\n",
    "\n",
    "### Get Synsets ###\n",
    "\n",
    "syn_words = [ wn.synsets(word) for word in lem] # all synsets for the words\n",
    "\n",
    "print(syn_words[1])\n",
    "\n",
    "### Get  lemma names for the synsets from the previous code in Danish ###\n",
    "lem_w = []\n",
    "for i in syn_words:\n",
    "    #print(i)\n",
    "    for j in i:\n",
    "        lem_w.append(j.lemma_names('dan'))\n",
    "ind_no_syn = [] # list of all index words that have no lemmas\n",
    "   \n",
    "for i,j in enumerate(lem_w):  # stores the indexes for the words that dont have target lemmas\n",
    "    if j == [] :\n",
    "        ind_no_syn.append(i) \n",
    "    \n",
    "    lem_w_clean = [x for x in lem_w if x != []]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### Create list that has only words with target lemmas ### \n",
    "word_with_lemmas = [x for x in lem if lem.index(x) not in ind_no_syn] # creates a list of all the words that have danish lemmas\n",
    "#print(word_with_lemmas)\n",
    "\n",
    "\n",
    "### Take random sample ### \n",
    "random_five = random.sample(word_with_lemmas,5) # take random sample of words that have target lemmas\n",
    "\n",
    "print(random_five)\n",
    "\n",
    "\n",
    "### get danish translation for the words ###\n",
    "\n",
    "\n",
    "exp_1 = ['latch', 'suppose', 'take', 'afraid', 'got']\n",
    "\n",
    "exp_2 = ['saw', 'got', 'anything', 'suppose', 'open']\n",
    "\n",
    "syns_exp1 = [wn.synsets(x) for x in exp_1]\n",
    "exp_1_lemmas = []\n",
    "\n",
    "for i in syns_exp1:\n",
    "    for j in i:\n",
    "        exp_1_lemmas.append(j.lemma_names('dan'))\n",
    "\n",
    "\n",
    "syns_exp2 = [wn.synsets(x) for x in exp_2]\n",
    "exp2_lemmas =[]\n",
    "\n",
    "for i in syns_exp2:\n",
    "    for j in i:\n",
    "        exp2_lemmas.append(j.lemma_names('dan'))\n",
    "\n",
    "\n",
    "#print(rlemmas)\n",
    "\n",
    "\n",
    "### More general code for checking any random 5 ###\n",
    "\n",
    "syns_rand = [wn.synsets(x) for x in random_five]\n",
    "rand_lemmas = []\n",
    "for i in syns_rand:\n",
    "    for j in i:\n",
    "        rand_lemmas.append(j.lemma_names('dan'))\n",
    "\n",
    "rand_lemmas = [rand_lemmas[x] for x in range(len(rand_lemmas)) if rand_lemmas[x] != []]\n",
    "\n",
    "print(\"Random\", rand_lemmas)\n",
    "\"\"\" Experiment 1 \n",
    "\n",
    "     random_five = ['latch', 'suppose', 'take', 'afraid', 'got']\n",
    "     \n",
    "     in danish ( from google translate): \"låsen, formode, tage, bange, fik \"\n",
    "        \n",
    "     the only lemma matches for exp1 was bange.\n",
    "     \n",
    "    Exp 2\n",
    "    \n",
    "     random_five = 'saw', 'got', 'anything', 'suppose', 'open'\n",
    "     \n",
    "     in danish = :sav, fik, hvad som helst, formode, aben\n",
    "\n",
    "     The lemmas that mached were save for saw, and aben for open. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
